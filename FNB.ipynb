{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeKLA9A7DKsSCewNXEAoXl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ElvPO8ryoE66","executionInfo":{"status":"error","timestamp":1681993546680,"user_tz":-330,"elapsed":1642,"user":{"displayName":"SOUMEN MONDAL","userId":"15608075891583984647"}},"outputId":"73360868-bc42-4ba0-fd1c-4ea407878371","colab":{"base_uri":"https://localhost:8080/","height":374}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ae7b1218854f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas_ml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPassiveAggressiveClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas_ml'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["#--------------------------------------------------------------\n","# Include Libraries\n","#--------------------------------------------------------------\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import sklearn\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn import metrics\n","from pandas_ml import ConfusionMatrix\n","from matplotlib import pyplot as plt\n","from sklearn.linear_model import PassiveAggressiveClassifier\n","from sklearn.feature_extraction.text import HashingVectorizer\n","import itertools\n","import numpy as np\n","\n","\n","#--------------------------------------------------------------\n","# Importing dataset using pandas dataframe\n","#--------------------------------------------------------------\n","df = pd.read_csv(\"fake_or_real_news.csv\")\n","    \n","# Inspect shape of `df` \n","df.shape\n","\n","# Print first lines of `df` \n","df.head()\n","\n","# Set index \n","df = df.set_index(\"Unnamed: 0\")\n","\n","# Print first lines of `df` \n","df.head()\n","\n","\n","#--------------------------------------------------------------\n","# Separate the labels and set up training and test datasets\n","#--------------------------------------------------------------\n","y = df.label \n","\n","# Drop the `label` column\n","df.drop(\"label\", axis=1)      #where numbering of news article is done that column is dropped in dataset\n","\n","# Make training and test sets \n","X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n","\n","\n","#--------------------------------------------------------------\n","# Building the Count and Tfidf Vectors\n","#--------------------------------------------------------------\n","\n","# Initialize the `count_vectorizer` \n","count_vectorizer = CountVectorizer(stop_words='english')\n","\n","# Fit and transform the training data \n","count_train = count_vectorizer.fit_transform(X_train)                  # Learn the vocabulary dictionary and return term-document matrix.\n","\n","# Transform the test set \n","count_test = count_vectorizer.transform(X_test)\n","\n","# Initialize the `tfidf_vectorizer` \n","tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)    # This removes words which appear in more than 70% of the articles\n","\n","# Fit and transform the training data \n","tfidf_train = tfidf_vectorizer.fit_transform(X_train) \n","\n","# Transform the test set \n","tfidf_test = tfidf_vectorizer.transform(X_test)\n","\n","# Get the feature names of `tfidf_vectorizer` \n","#print(tfidf_vectorizer.get_feature_names()[-10:])\n","\n","# Get the feature names of `count_vectorizer` \n","#print(count_vectorizer.get_feature_names()[:10])\n","\n","count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n","\n","tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n","\n","difference = set(count_df.columns) - set(tfidf_df.columns)\n","\n","print(difference)\n","\n","# Check whether the DataFrames are equal\n","print(count_df.equals(tfidf_df))\n","\n","print(count_df.head())\n","\n","print(tfidf_df.head())\n","\n","\n","#--------------------------------------------------------------\n","# Function to plot the confusion matrix \n","#--------------------------------------------------------------\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    See full source and example: \n","    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n","    \n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","\n","#--------------------------------------------------------------\n","# Naive Bayes classifier for Multinomial model \n","#--------------------------------------------------------------\n","\n","clf = MultinomialNB() \n","\n","clf.fit(tfidf_train, y_train)                       # Fit Naive Bayes classifier according to X, y\n","\n","pred = clf.predict(tfidf_test)                     # Perform classification on an array of test vectors X.\n","score = metrics.accuracy_score(y_test, pred)\n","print(\"accuracy:   %0.3f\" % score)\n","cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n","plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])\n","print(cm)\n","\n","\n","clf = MultinomialNB()\n","\n","clf.fit(count_train, y_train)\n","\n","pred = clf.predict(count_test)\n","score = metrics.accuracy_score(y_test, pred)\n","print(\"accuracy:   %0.3f\" % score)\n","cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n","plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])\n","print(cm)\n","\n","\n","#--------------------------------------------------------------\n","# Applying Passive Aggressive Classifier\n","#--------------------------------------------------------------\n","\n","linear_clf = PassiveAggressiveClassifier(n_iter=50)\n","\n","linear_clf.fit(tfidf_train, y_train)\n","pred = linear_clf.predict(tfidf_test)\n","score = metrics.accuracy_score(y_test, pred)\n","print(\"accuracy:   %0.3f\" % score)\n","cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n","plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])\n","print(cm)\n","\n","\n","clf = MultinomialNB(alpha=0.1)               # Additive (Laplace/Lidstone) smoothing parameter\n","\n","last_score = 0\n","for alpha in np.arange(0,1,.1):\n","    nb_classifier = MultinomialNB(alpha=alpha)\n","    nb_classifier.fit(tfidf_train, y_train)\n","    pred = nb_classifier.predict(tfidf_test)\n","    score = metrics.accuracy_score(y_test, pred)\n","    if score > last_score:\n","        clf = nb_classifier\n","    print(\"Alpha: {:.2f} Score: {:.5f}\".format(alpha, score))\n","\n","\n","def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):       # inspect the top 30 vectors for fake and real news\n","    \"\"\"\n","    See: https://stackoverflow.com/a/26980472\n","    \n","    Identify most important features if given a vectorizer and binary classifier. Set n to the number\n","    of weighted features you would like to show. (Note: current implementation merely prints and does not \n","    return top classes.)\n","    \"\"\"\n","\n","    class_labels = classifier.classes_\n","    feature_names = vectorizer.get_feature_names()                                            # Array mapping from feature integer indices to feature name\n","    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n","    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n","\n","    for coef, feat in topn_class1:\n","        print(class_labels[0], coef, feat)\n","\n","    print()\n","\n","    for coef, feat in reversed(topn_class2):\n","        print(class_labels[1], coef, feat)\n","\n","\n","most_informative_feature_for_binary_classification(tfidf_vectorizer, linear_clf, n=30)\n","feature_names = tfidf_vectorizer.get_feature_names()\n","\n","### Most real\n","sorted(zip(clf.coef_[0], feature_names), reverse=True)[:20]\n","\n","### Most fake\n","sorted(zip(clf.coef_[0], feature_names))[:20]                               # clearly there are certain words which might show political intent and source in the top fake features (such as the words corporate and establishment).\n","\n","tokens_with_weights = sorted(list(zip(feature_names, clf.coef_[0])))\n","#print(tokens_with_weights)\n","\n","#--------------------------------------------------------------\n","# HashingVectorizer : require less memory and are faster (because they are sparse and use hashes rather than tokens)\n","#--------------------------------------------------------------\n","\n","\n","hash_vectorizer = HashingVectorizer(stop_words='english', non_negative=True)\n","hash_train = hash_vectorizer.fit_transform(X_train)\n","hash_test = hash_vectorizer.transform(X_test)\n","\n","#--------------------------------------------------------------\n","# Naive Bayes classifier for Multinomial model \n","#-------------------------------------------------------------- \n","\n","clf = MultinomialNB(alpha=.01)\n","\n","clf.fit(hash_train, y_train)\n","pred = clf.predict(hash_test)\n","score = metrics.accuracy_score(y_test, pred)\n","print(\"accuracy:   %0.3f\" % score)\n","cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n","plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])\n","print(cm)\n","\n","\n","#--------------------------------------------------------------\n","# Applying Passive Aggressive Classifier\n","#--------------------------------------------------------------\n","\n","clf = PassiveAggressiveClassifier(n_iter=50)    \n","\n","clf.fit(hash_train, y_train)\n","pred = clf.predict(hash_test)\n","score = metrics.accuracy_score(y_test, pred)\n","print(\"accuracy:   %0.3f\" % score)\n","cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n","plot_confusion_matrix(cm, classes=['FAKE', 'REAL'])\n","print(cm)"]},{"cell_type":"code","source":[],"metadata":{"id":"dZEyH7j0oGK1"},"execution_count":null,"outputs":[]}]}